{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to explore the encoded vector of the nnUNet to create an decoder to regress / classify the output instead of segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebooks\n",
      "Added path to sys\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "print(os.getcwd().split('/')[-1])\n",
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    sys.path.append('../pytorch')\n",
    "    print('Added path to sys')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the nnUNet model\n",
    "from utils import KFoldNNUNetSegmentationDataModule, GenesisSegmentation\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "# Load the config file\n",
    "config = yaml.load(open('../pytorch/configs/fine_tune_config-regression.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "# Model\n",
    "model = GenesisSegmentation(config=config).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the encoder\n",
    "encoder = model.model.encoder \n",
    "\n",
    "input_sample = torch.randn(1, 1, *config['data']['patch_size'])\n",
    "output_sample = encoder(input_sample)\n",
    "[feat.shape for feat in output_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to know if the Whole-Volumetric-OCT-Image can be cropped to a smaller size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def get_positive_bounds(image):\n",
    "    image = sitk.GetArrayFromImage(image)\n",
    "    zs, xs, ys = np.where(image > 0)\n",
    "    for z, x, y in zip(zs, xs, ys):\n",
    "        assert image[z, x, y] > 0\n",
    "\n",
    "    return zs.min(), zs.max(), xs.min(), xs.max(), ys.min(), ys.max()\n",
    "\n",
    "max_z = -1\n",
    "min_z = 1000\n",
    "max_x = -1\n",
    "min_x = 1000\n",
    "max_y = -1\n",
    "min_y = 1000\n",
    "\n",
    "min_img_z, max_img_z, min_img_x, max_img_x, min_img_y, max_img_y = 1000, -1, 1000, -1, 1000, -1\n",
    "\n",
    "data_dir = Path('/storage_bizon/naravich/nnUNet_Datasets/nnUNet_raw/Dataset303_Calcium_OCTv2/imagesTr/').glob('*.nii.gz')\n",
    "label_dir = Path('/storage_bizon/naravich/nnUNet_Datasets/nnUNet_raw/Dataset303_Calcium_OCTv2/labelsTr/').glob('*.nii.gz')\n",
    "print('train')\n",
    "for file in data_dir:\n",
    "    img = sitk.ReadImage(str(file))\n",
    "    print(img.GetSize(), file.stem)\n",
    "    positive_bounds = get_positive_bounds(img)\n",
    "    min_img_z = min(min_img_z, positive_bounds[0])\n",
    "    max_img_z = max(max_img_z, positive_bounds[1])\n",
    "    min_img_x = min(min_img_x, positive_bounds[2])\n",
    "    max_img_x = max(max_img_x, positive_bounds[3])\n",
    "    min_img_y = min(min_img_y, positive_bounds[4])\n",
    "    max_img_y = max(max_img_y, positive_bounds[5])\n",
    "\n",
    "print('label')\n",
    "for file in label_dir:\n",
    "    img = sitk.ReadImage(str(file))\n",
    "    print(img.GetSize(), file.stem)\n",
    "    positive_bounds = get_positive_bounds(img)\n",
    "    min_z = min(min_z, positive_bounds[0])\n",
    "    max_z = max(max_z, positive_bounds[1])\n",
    "    min_x = min(min_x, positive_bounds[2])\n",
    "    max_x = max(max_x, positive_bounds[3])\n",
    "    min_y = min(min_y, positive_bounds[4])\n",
    "    max_y = max(max_y, positive_bounds[5])\n",
    "\n",
    "data_dir = Path('/storage_bizon/naravich/nnUNet_Datasets/nnUNet_raw/Dataset303_Calcium_OCTv2/imagesTs/').glob('*.nii.gz')\n",
    "label_dir = Path('/storage_bizon/naravich/nnUNet_Datasets/nnUNet_raw/Dataset303_Calcium_OCTv2/labelsTs/').glob('*.nii.gz')\n",
    "print('test')\n",
    "for file in data_dir:\n",
    "    img = sitk.ReadImage(str(file))\n",
    "    print(img.GetSize(), file.stem)\n",
    "    positive_bounds = get_positive_bounds(img)\n",
    "    min_img_z = min(min_img_z, positive_bounds[0])\n",
    "    max_img_z = max(max_img_z, positive_bounds[1])\n",
    "    min_img_x = min(min_img_x, positive_bounds[2])\n",
    "    max_img_x = max(max_img_x, positive_bounds[3])\n",
    "    min_img_y = min(min_img_y, positive_bounds[4])\n",
    "    max_img_y = max(max_img_y, positive_bounds[5])\n",
    "\n",
    "for file in label_dir:\n",
    "    img = sitk.ReadImage(str(file))\n",
    "    print(img.GetSize(), file.stem)\n",
    "    positive_bounds = get_positive_bounds(img)\n",
    "    min_z = min(min_z, positive_bounds[0])\n",
    "    max_z = max(max_z, positive_bounds[1])\n",
    "    min_x = min(min_x, positive_bounds[2])\n",
    "    max_x = max(max_x, positive_bounds[3])\n",
    "    min_y = min(min_y, positive_bounds[4])\n",
    "    max_y = max(max_y, positive_bounds[5])\n",
    "\n",
    "print('Minimum croppable bound is: ')\n",
    "print(min_z, max_z, min_x, max_x, min_y, max_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the image data, the image is 3D and the size cannot be cropped to a smaller size. As for the label, it can be cropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UNetRegressor\n",
    "class UNetRegressor(torch.nn.Module):\n",
    "    def __init__(self, n_classes: int, task: str = 'regression'):\n",
    "        super(UNetRegressor, self).__init__()\n",
    "        self.encoder = model.model.encoder\n",
    "        \"\"\"\n",
    "        >>> input_sample = torch.randn(1, 1, 512, 512, 384)\n",
    "        >>> output_sample = encoder(input_sample)\n",
    "        >>> [feat.shape for feat in output_sample]\n",
    "        >>> [torch.Size([1, 32, 512, 512, 384]),\n",
    "             torch.Size([1, 64, 256, 256, 192]),\n",
    "             torch.Size([1, 128, 128, 128, 96]),\n",
    "             torch.Size([1, 256, 64, 64, 48]),\n",
    "             torch.Size([1, 320, 32, 32, 24]),\n",
    "             torch.Size([1, 320, 32, 16, 12])]\n",
    "        \"\"\"\n",
    "        # self.decoder = model.model.decoder\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(320, 320, kernel_size=(32, 16, 12), stride=1, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv3d(320, 160, kernel_size=1, stride=1, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv3d(160, n_classes, kernel_size=1, stride=1, bias=True),\n",
    "        )\n",
    "        self.last_activation = torch.nn.Sigmoid() if task == 'regression' else torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)[-1]\n",
    "        x = self.regressor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UNetRegressor\n",
    "class UNetRegressorHead(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, n_classes: int, pooling=\"avg\", dropout=0.2, task: str = 'regression'):\n",
    "        super(UNetRegressorHead, self).__init__()\n",
    "        \"\"\"\n",
    "        >>> input_sample = torch.randn(1, 1, 512, 512, 384)\n",
    "        >>> output_sample = encoder(input_sample)\n",
    "        >>> [feat.shape for feat in output_sample]\n",
    "        >>> [torch.Size([1, 32, 512, 512, 384]),\n",
    "             torch.Size([1, 64, 256, 256, 192]),\n",
    "             torch.Size([1, 128, 128, 128, 96]),\n",
    "             torch.Size([1, 256, 64, 64, 48]),\n",
    "             torch.Size([1, 320, 32, 32, 24]),\n",
    "             torch.Size([1, 320, 32, 16, 12])]\n",
    "        \"\"\"\n",
    "        if pooling not in (\"max\", \"avg\"):\n",
    "            raise ValueError(\"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling))\n",
    "\n",
    "        if task not in (\"regression\", \"classification\"):\n",
    "            raise ValueError(\"Task should be one of ('regression', 'classification'), got {}.\".format(task))\n",
    "\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.AdaptiveAvgPool3d(1) if pooling == \"avg\" else torch.nn.AdaptiveMaxPool3d(1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Dropout(p=dropout, inplace=True) if dropout > 0 else torch.nn.Identity(),\n",
    "            torch.nn.Linear(in_channels, n_classes, bias=True),\n",
    "            torch.nn.ReLU() if task == 'regression' else torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.regressor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.model.encoder\n",
    "input_sample = torch.randn(10, 1, *config['data']['patch_size'])\n",
    "output_sample = encoder(input_sample)\n",
    "regressor = UNetRegressorHead(in_channels=320, n_classes=2, pooling=\"avg\", dropout=0.2, task='classification')\n",
    "output_sample = regressor(output_sample[-1])\n",
    "output_sample.shape, output_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "unlabeled_data_dir = Path('/storage_bizon/naravich/Unlabeled_OCT_by_CADx/') # Yiqing filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ivl = pd.read_excel('tabular_data/T1A_PRE_QUANT_LESION.xlsx', skiprows=4)\n",
    "pre_ivl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename columns to their code names `FLAG_CAL = Flag for calcified nodules` -> `FLAG_CAL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = pre_ivl.columns\n",
    "column_names = {column_name:column_name.split(' = ')[0] for column_name in column_names}\n",
    "column_names\n",
    "pre_ivl.rename(columns=column_names, inplace=True)\n",
    "pre_ivl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Unique Subject ID (USUBJID) to the format we use in the dataset `CP 61774-105-001` -> `105-001`\n",
    "That is [A-Z]{2} [0-9]{5}-[0-9]{3}-[0-3]{3} -> [0-9]{3}-[0-9]{3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "\n",
    "def format_subject_id(subject_id: str):\n",
    "    # Define the pattern\n",
    "    pattern = r\"[A-Z]{2} [0-9]{4,5}-([0-9]{3}-[0-9]{3})\"\n",
    "\n",
    "    # Compile the pattern\n",
    "    regex = re.compile(pattern)\n",
    "\n",
    "    # Search for the pattern in the input string\n",
    "    match = regex.search(unidecode.unidecode(subject_id))\n",
    "\n",
    "    # If a match is found, extract the desired part using the replacement pattern\n",
    "    if match:\n",
    "        extracted_part = re.sub(r\".*?([0-9]{3}-[0-9]{3})\", r\"\\1\", match.group(1))\n",
    "        return extracted_part\n",
    "    raise ValueError(f\"No match found {subject_id.strip().replace(u'\\xa0', ' ')}\")\n",
    "\n",
    "pre_ivl['USUBJID'].apply(lambda x: format_subject_id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ivl['USUBJID'] = pre_ivl['USUBJID'].apply(lambda x: format_subject_id(x))\n",
    "pre_ivl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only the columns we are interested in\n",
    "\n",
    "```\n",
    "MLAS_AS = Area stenosis (%)\t\n",
    "MLAS_SCA = Superficial calcium arc (°)\t\n",
    "MLAS_MSCT = Maximum superficial calcium thickness (mm)\t\n",
    "MCS_LA = Maximum calcium site -  Lumen area (mm2)\t\n",
    "MCS_AS = Maximum calcium site - Area stenosis (%)\t\n",
    "MCS_SCA = Maximum calcium site -  Superficial calcium arc (°)\t\n",
    "MCS_MSCT = Maximum calcium site -   Maximum superficial calcium thickness (mm)\t\n",
    "MCCS_LA = Maximum continuous calcium site - Lumen area (mm2)\t\n",
    "MCCS_AS = Maximum continuous calcium site - Area stenosis (%)\t\n",
    "MCCS_SCA = Maximum continuous calcium site - Superficial calcium arc (°)\t\n",
    "MCCS_MSCT = Maximum continuous calcium site - Maximum superficial calcium thickness (mm)\t\n",
    "MCCS_MINSCT = Maximum continuous calcium site - Minimum superficial calcium thickness (mm)\t\n",
    "MCCS_CSCA = Maximum continuous calcium site - Circumferential superficial calcium\t\n",
    "MCCS_CSCA_270 = Maximum continuous calcium site - Length of circumferential superficial calcium greater than equal to 270(mm)\t\n",
    "MCCS_CSCA_180 = Maximum continuous calcium site - Length of circumferential superficial calcium greater than equal to 180(mm)\n",
    "FMSA_LA = Final minimum stent area site - Lumen area (mm2)\t\n",
    "FMSA_AS = Final minimum stent area site - Area stenosis (%)\t\n",
    "FMSA_SCA = Final minimum stent area site - Superficial calcium arc (°)\t\n",
    "FMSA_MSCT = Final minimum stent area site - Maximum superficial calcium thickness (mm)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    'USUBJID',\n",
    "    'STUDY',\n",
    "    'MLAS_AS',\n",
    "    'MLAS_SCA',\n",
    "    'MLAS_MSCT',\n",
    "    'MCS_LA',\n",
    "    'MCS_AS',\n",
    "    'MCS_SCA',\n",
    "    'MCS_MSCT',\n",
    "    'MCCS_LA',\n",
    "    'MCCS_AS',\n",
    "    'MCCS_SCA',\n",
    "    'MCCS_MSCT',\n",
    "    'MCCS_MINSCT',\n",
    "    'MCCS_CSCA',\n",
    "    'MCCS_CSCA_270',\n",
    "    'MCCS_CSCA_180',\n",
    "    'FMSA_LA',\n",
    "    'FMSA_AS',\n",
    "    'FMSA_SCA',\n",
    "    'FMSA_MSCT',\n",
    "]\n",
    "\n",
    "pre_ivl = pre_ivl[selected_columns]\n",
    "pre_ivl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just to make every easy for the dataloader, we will put the absolute path of the image to the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all the images are named with 'Final', 'Pre', or 'Post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = unlabeled_data_dir / 'NiFTI'\n",
    "image_path = image_path.glob('*.nii.gz')\n",
    "for i in image_path:\n",
    "    if 'Final' in i.stem or 'Pre' in i.stem or 'Post' in i.stem:\n",
    "        continue\n",
    "    else:\n",
    "        print(i.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_image_path(subject_id: str):\n",
    "    image_path = unlabeled_data_dir / 'NiFTI'\n",
    "    image_path = image_path.glob(\"{}Pre*\".format(subject_id.replace('-', '')))\n",
    "    image_path = list(image_path)\n",
    "    if not image_path:\n",
    "        return None\n",
    "    return image_path[0]\n",
    "\n",
    "pre_ivl['image_path'] = pre_ivl['USUBJID'].apply(lambda x: resolve_image_path(x))\n",
    "pre_ivl.dropna(subset=['image_path'], inplace=True)\n",
    "pre_ivl.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly fill . with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pre_ivl.replace({'\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0.': np.nan}, inplace=False).to_csv('tabular_data/pre_ivl.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same thing for post stent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "post_stent = pd.read_excel('tabular_data/T6_POST_STENT_LESION.xlsx', skiprows=4)\n",
    "\n",
    "column_names = post_stent.columns\n",
    "column_names = {column_name:column_name.split(' = ')[0] for column_name in column_names}\n",
    "\n",
    "post_stent.rename(columns=column_names, inplace=True)\n",
    "\n",
    "\n",
    "def format_subject_id(subject_id: str):\n",
    "    # Define the pattern\n",
    "    pattern = r\"[A-Z]{2} [0-9]{4,5}-([0-9]{3}-[0-9]{3})\"\n",
    "\n",
    "    # Compile the pattern\n",
    "    regex = re.compile(pattern)\n",
    "\n",
    "    # Search for the pattern in the input string\n",
    "    match = regex.search(unidecode.unidecode(subject_id))\n",
    "\n",
    "    # If a match is found, extract the desired part using the replacement pattern\n",
    "    if match:\n",
    "        extracted_part = re.sub(r\".*?([0-9]{3}-[0-9]{3})\", r\"\\1\", match.group(1))\n",
    "        return extracted_part\n",
    "    raise ValueError(f\"No match found {subject_id.strip().replace(u'\\xa0', ' ')}\")\n",
    "\n",
    "post_stent['USUBJID'] = post_stent['USUBJID'].apply(lambda x: format_subject_id(x))\n",
    "\n",
    "selected_columns = [\n",
    "    'USUBJID',\n",
    "    'STUDY',\n",
    "\n",
    "    # Categorical\n",
    "    'MAL_PRES',\n",
    "    'MAL_PROX',\n",
    "    'MAL_DIS',\n",
    "    'MAL_SBOD',\n",
    "    'MMAL_CF',\n",
    "    'MMAL_NCF',\n",
    "    'CF_PRES',\n",
    "    'CF_3',\n",
    "    'CF_2',\n",
    "    'CF_1',\n",
    "\n",
    "    # Morphological\n",
    "    'MMAL_SA',\n",
    "    'MMAL_LA',\n",
    "    'MMAL_ARC',\n",
    "    'MMAL_AR',\n",
    "    'MMAL_PAR',\n",
    "    'MAL_LEN',\n",
    "    'MAL_THICK',\n",
    "    'TOT_CAL0',\n",
    "    'TOT_CAL',\n",
    "    'MEAN_CF',\n",
    "    'MAX_CF',\n",
    "    'TOT_CFLEN',\n",
    "    'MAX_CFDEP',\n",
    "    'MAX_CFWID',\n",
    "    'MAX_CFTHK',\n",
    "    'MAX_CARC',\n",
    "    'MIN_CARC',\n",
    "]\n",
    "\n",
    "post_stent = post_stent[selected_columns]\n",
    "\n",
    "image_path = unlabeled_data_dir / 'NiFTI'\n",
    "image_path = image_path.glob('*.nii.gz')\n",
    "for i in image_path:\n",
    "    if 'Final' in i.stem or 'Pre' in i.stem or 'Post' in i.stem:\n",
    "        continue\n",
    "    else:\n",
    "        print(i.stem)\n",
    "\n",
    "def resolve_image_path(subject_id: str):\n",
    "    image_path = unlabeled_data_dir / 'NiFTI'\n",
    "    image_path = image_path.glob(\"{}Final*\".format(subject_id.replace('-', '')))\n",
    "    image_path = list(image_path)\n",
    "    if not image_path:\n",
    "        return None\n",
    "    return image_path[0]\n",
    "\n",
    "post_stent['image_path'] = post_stent['USUBJID'].apply(lambda x: resolve_image_path(x))\n",
    "post_stent.dropna(subset=['image_path'], inplace=True)\n",
    "\n",
    "post_stent.replace({'\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0.': np.nan}, inplace=False).to_csv('tabular_data/Post_Stent.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build the dataloader\n",
    "Let's start with Pre-IVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from utils import KFoldNNUNetTabularDataModule\n",
    "\n",
    "with open('../pytorch/configs/fine_tune_config-regression.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "dataModule = KFoldNNUNetTabularDataModule(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Subject(Keys: ('MCS_MSCT', 'image', 'location'); images: 1), Subject(Keys: ('MCS_MSCT', 'image', 'location'); images: 1)]\n",
      "dict_keys(['MCS_MSCT', 'image', 'location'])\n"
     ]
    }
   ],
   "source": [
    "dataModule.setup('fit')\n",
    "train_dataloader = dataModule.train_dataloader()\n",
    "for batch in train_dataloader:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['MCS_MSCT', 'image'])\n",
      "torch.Size([2, 1, 512, 512, 384])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(batch.keys())\n",
    "print(batch['image'].shape)\n",
    "print(batch['MCS_MSCT'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in config['data']['output_metrics']:\n",
    "    print(metric, batch[0][metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0]['image'][tio.DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "for batch in train_dataloader:\n",
    "    batches.append(batch)\n",
    "    if len(batches) == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches[1][0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in config['data']['output_metrics']:\n",
    "    print(metric, batches[1][0][metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pre_ivl = pd.read_csv('tabular_data/Pre_IVL.csv')\n",
    "post_ivl = pd.read_csv('tabular_data/Post_Stent.csv')\n",
    "post_stent = pd.read_csv('tabular_data/Post_Stent.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data counting percentage of non-NA values in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * pre_ivl.count(axis=0) / pre_ivl.shape[0]\n",
    "\n",
    "# data = [\n",
    "#     'MCS_LA',\n",
    "#     'MCS_SCA',\n",
    "#     'MCS_MSCT',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * post_ivl.count(axis=0) / pre_ivl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * post_stent.count(axis=0) / pre_ivl.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting # of images in modality A available as inputs to predict metrics of modality B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_modality, output_modality_name in zip([pre_ivl, post_ivl, post_stent], ['Pre_IVL', 'Post_IVL', 'Post_Stent']):\n",
    "    for input_modality in ['Pre_IVL', 'Post_IVL', 'Post_Stent']:\n",
    "        count = output_modality[f'{input_modality}_image_path'].count()\n",
    "        total = output_modality.shape[0]\n",
    "        percentage = 100 * count / total\n",
    "        print(f'{input_modality} -> {output_modality_name}')\n",
    "        print(f'  {count} / {total} = {percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the distribution of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as  np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ivl.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting 3 columns that have the least NA-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MLAS_AS = Area stenosis (%)\t\n",
    "MLAS_SCA = Superficial calcium arc (°)\t\n",
    "MLAS_MSCT = Maximum superficial calcium thickness (mm)\t\n",
    "MCS_LA = Maximum calcium site -  Lumen area (mm2)\t\n",
    "MCS_AS = Maximum calcium site - Area stenosis (%)\t\n",
    "MCS_SCA = Maximum calcium site -  Superficial calcium arc (°)\t\n",
    "MCS_MSCT = Maximum calcium site -   Maximum superficial calcium thickness (mm)\t\n",
    "MCCS_LA = Maximum continuous calcium site - Lumen area (mm2)\t\n",
    "MCCS_AS = Maximum continuous calcium site - Area stenosis (%)\t\n",
    "MCCS_SCA = Maximum continuous calcium site - Superficial calcium arc (°)\t\n",
    "MCCS_MSCT = Maximum continuous calcium site - Maximum superficial calcium thickness (mm)\t\n",
    "MCCS_MINSCT = Maximum continuous calcium site - Minimum superficial calcium thickness (mm)\t\n",
    "MCCS_CSCA = Maximum continuous calcium site - Circumferential superficial calcium\t\n",
    "MCCS_CSCA_270 = Maximum continuous calcium site - Length of circumferential superficial calcium greater than equal to 270(mm)\t\n",
    "MCCS_CSCA_180 = Maximum continuous calcium site - Length of circumferential superficial calcium greater than equal to 180(mm)\n",
    "FMSA_LA = Final minimum stent area site - Lumen area (mm2)\t\n",
    "FMSA_AS = Final minimum stent area site - Area stenosis (%)\t\n",
    "FMSA_SCA = Final minimum stent area site - Superficial calcium arc (°)\t\n",
    "FMSA_MSCT = Final minimum stent area site - Maximum superficial calcium thickness (mm)\n",
    "'''\n",
    "\n",
    "sns.pairplot(pre_ivl[[\n",
    "    'MCS_LA',\n",
    "    'MCS_SCA',\n",
    "    'MCS_MSCT',\n",
    "]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually there is no outliter in the data, so it is safe to use the min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_norm = lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "    \n",
    "min_max_normed_pre_ivl = pre_ivl[[\n",
    "    'MCS_LA',\n",
    "    'MCS_SCA',\n",
    "    'MCS_MSCT',\n",
    "]]\n",
    "min_max_normed_pre_ivl = min_max_normed_pre_ivl.apply(min_max_norm, axis=0)\n",
    "sns.pairplot(\n",
    "    min_max_normed_pre_ivl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_norm = lambda x: (x - x.mean()) / (x.std())\n",
    "    \n",
    "z_normed_pre_ivl = pre_ivl[[\n",
    "    'MCS_LA',\n",
    "    'MCS_SCA',\n",
    "    'MCS_MSCT',\n",
    "]]\n",
    "z_normed_pre_ivl = z_normed_pre_ivl.apply(z_norm, axis=0)\n",
    "sns.pairplot(\n",
    "    z_normed_pre_ivl,\n",
    "    corner=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_norm = lambda x: np.log(x)\n",
    "    \n",
    "log_normed_pre_ivl = pre_ivl[[\n",
    "    'MCS_LA',\n",
    "    'MCS_SCA',\n",
    "    'MCS_MSCT',\n",
    "]]\n",
    "log_normed_pre_ivl = log_normed_pre_ivl.apply(log_norm, axis=0)\n",
    "sns.pairplot(\n",
    "    log_normed_pre_ivl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ivl.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
