{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robust_pca(features: torch.Tensor, m: float = 2, remove_first_component=False):\n",
    "    # features: (N, C)\n",
    "    # m: a hyperparam controlling how many std dev outside for outliers\n",
    "    assert len(features.shape) == 2, \"features should be (N, C)\"\n",
    "    reduction_mat = torch.pca_lowrank(features, q=3, niter=20)[2]\n",
    "    colors = features @ reduction_mat\n",
    "    if remove_first_component:\n",
    "        colors_min = colors.min(dim=0).values\n",
    "        colors_max = colors.max(dim=0).values\n",
    "        tmp_colors = (colors - colors_min) / (colors_max - colors_min)\n",
    "        fg_mask = tmp_colors[..., 0] < 0.2\n",
    "        # cv2.imwrite('fg_mask.png', fg_mask.reshape(32, 32).numpy().astype(np.uint8) * 255)\n",
    "        # fg_mask2 = tmp_colors[..., 1] < 0.2\n",
    "        # cv2.imwrite('fg_mask2.png', fg_mask2.reshape(32, 32).numpy().astype(np.uint8) * 255)\n",
    "        # fg_mask = (fg_mask & fg_mask2)\n",
    "        reduction_mat = torch.pca_lowrank(features[fg_mask], q=3, niter=20)[2]\n",
    "        colors = features @ reduction_mat\n",
    "    else:\n",
    "        fg_mask = torch.ones_like(colors[:, 0]).bool()\n",
    "    d = torch.abs(colors[fg_mask] - torch.median(colors[fg_mask], dim=0).values)\n",
    "    mdev = torch.median(d, dim=0).values\n",
    "    s = d / mdev\n",
    "    try:\n",
    "        rins = colors[fg_mask][s[:, 0] < m, 0]\n",
    "        gins = colors[fg_mask][s[:, 1] < m, 1]\n",
    "        bins = colors[fg_mask][s[:, 2] < m, 2]\n",
    "        rgb_min = torch.tensor([rins.min(), gins.min(), bins.min()])\n",
    "        rgb_max = torch.tensor([rins.max(), gins.max(), bins.max()])\n",
    "    except:\n",
    "        rins = colors\n",
    "        gins = colors\n",
    "        bins = colors\n",
    "        rgb_min = torch.tensor([rins.min(), gins.min(), bins.min()])\n",
    "        rgb_max = torch.tensor([rins.max(), gins.max(), bins.max()])\n",
    "\n",
    "    return reduction_mat, rgb_min.to(reduction_mat), rgb_max.to(reduction_mat)\n",
    "\n",
    "\n",
    "def get_pca_map(\n",
    "    feature_map: torch.Tensor,\n",
    "    img_size,\n",
    "    interpolation=\"bicubic\",\n",
    "    return_pca_stats=False,\n",
    "    pca_stats=None,\n",
    "    remove_first_component=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    feature_map: (1, h, w, C) is the feature map of a single image.\n",
    "    \"\"\"\n",
    "    # print(feature_map.shape)\n",
    "    if feature_map.shape[0] != 1:\n",
    "        # make it (1, h, w, C)\n",
    "        feature_map = feature_map[None]\n",
    "    if pca_stats is None:\n",
    "        reduct_mat, color_min, color_max = get_robust_pca(\n",
    "            feature_map.reshape(-1, feature_map.shape[-1]),\n",
    "            remove_first_component=remove_first_component,\n",
    "        )\n",
    "    else:\n",
    "        reduct_mat, color_min, color_max = pca_stats\n",
    "    pca_color = feature_map @ reduct_mat\n",
    "    pca_color = (pca_color - color_min) / (color_max - color_min)\n",
    "    pca_color = pca_color.clamp(0, 1)\n",
    "    pca_color = F.interpolate(\n",
    "        pca_color.permute(0, 3, 1, 2),\n",
    "        size=img_size,\n",
    "        mode=interpolation,\n",
    "    ).permute(0, 2, 3, 1)\n",
    "    pca_color = pca_color.cpu().numpy().squeeze(0)\n",
    "    if return_pca_stats:\n",
    "        return pca_color, (reduct_mat, color_min, color_max)\n",
    "    return pca_color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerModel, SegformerImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = SegformerImageProcessor.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\n",
    "model = SegformerModel.from_pretrained('nvidia/segformer-b5-finetuned-ade-640-640')\n",
    "\n",
    "image = cv2.imread('/home/naravich/projects/RADIO/vis_denoise/02005Pre/02005Pre_001.png', cv2.IMREAD_COLOR)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_size = (640, 640)\n",
    "image = cv2.resize(image, image_size)\n",
    "# image = image / 255.0\n",
    "\n",
    "images = processor(image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    output = model(**images)\n",
    "\n",
    "feat = output.last_hidden_state.cpu().permute(0, 2, 3, 1)\n",
    "print(feat.shape)\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "num_rows = num_cols = feat.shape[1]\n",
    "\n",
    "fig = plt.figure()\n",
    "axis = plt.axes() \n",
    "\n",
    "def animate(frame_number):\n",
    "    # kmeans = KMeans(n_clusters=frame_number + 1, random_state=0).fit(rearrange(feat, 'b h w c -> (b h w) c', h=num_rows, w=num_cols).numpy())\n",
    "    kmeans = KMeans(n_clusters=frame_number + 1, random_state=0).fit(feat.reshape(num_cols * num_rows, -1).numpy())\n",
    "\n",
    "    axis.set_title(f'K: {frame_number}')\n",
    "    im = axis.imshow(kmeans.labels_.reshape(num_rows, num_cols), interpolation='nearest', cmap='tab20')\n",
    "    return [im]\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=50, interval=20, blit=True) \n",
    "anim.save('kmeans_segformer_b5.mp4', writer='ffmpeg', fps=2)\n",
    "\n",
    "pca_map = get_pca_map(feat, image_size, interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_map = get_pca_map(feat, image_size, interpolation=\"bilinear\")\n",
    "pca_map_2 = get_pca_map(feat, image_size, interpolation=\"bilinear\", remove_first_component=True)\n",
    "\n",
    "sbs = np.concatenate([image / 255, pca_map, pca_map_2], axis=1)\n",
    "plt.imshow(sbs)\n",
    "# image.min(), image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunetv2.run.run_training import get_trainer_from_args\n",
    "\n",
    "trainer = get_trainer_from_args(dataset_name_or_id=\"302\", configuration=\"2d\", fold=0, trainer_name=\"nnUNetTrainer\", plans_identifier=\"nnUNetPlans\")\n",
    "trainer.load_checkpoint('/storage_bizon/naravich/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__2d/fold_0/checkpoint_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "device = torch.device('cuda')\n",
    "image = cv2.imread('/home/naravich/projects/RADIO/vis_denoise/02005Pre/02005Pre_001.png', cv2.IMREAD_COLOR)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_size = (512, 512)\n",
    "image = cv2.resize(image, image_size)\n",
    "images = image[:, :, 1]\n",
    "images = images / 255.0\n",
    "images = images - images.mean()\n",
    "images /= images.std()\n",
    "print(images.min(), images.max())\n",
    "images = images[np.newaxis, ...]\n",
    "images = torch.tensor(images, device='cpu', dtype=torch.float32)\n",
    "\n",
    "plt.imshow(images.permute(1, 2, 0).cpu().numpy())\n",
    "print(images.shape)\n",
    "with torch.no_grad():\n",
    "    output = trainer.network.encoder(images.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs = None\n",
    "for i in range(len(output)):\n",
    "    feat = output[i].cpu().permute(0, 2, 3, 1)\n",
    "    pca_map = get_pca_map(feat, image_size, interpolation=\"bilinear\")\n",
    "    pca_map_2 = get_pca_map(feat, image_size, interpolation=\"bilinear\", remove_first_component=True)\n",
    "\n",
    "    sbs_tmp = np.concatenate([image / 255, pca_map, pca_map_2], axis=1)\n",
    "    sbs = sbs_tmp if sbs is None else np.concatenate([sbs, sbs_tmp], axis=0)\n",
    "fig, ax = plt.subplots(1, figsize=(20, 50))\n",
    "ax.imshow(sbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "feat = output[4].cpu().permute(0, 2, 3, 1)\n",
    "num_rows = num_cols = feat.shape[1]\n",
    "\n",
    "fig = plt.figure()\n",
    "axis = plt.axes() \n",
    "\n",
    "def animate(frame_number):\n",
    "    # kmeans = KMeans(n_clusters=frame_number + 1, random_state=0).fit(rearrange(feat, 'b h w c -> (b h w) c', h=num_rows, w=num_cols).numpy())\n",
    "    kmeans = KMeans(n_clusters=frame_number + 1, random_state=0).fit(feat.reshape(num_cols * num_rows, -1).numpy())\n",
    "\n",
    "    axis.set_title(f'K: {frame_number}')\n",
    "    im = axis.imshow(kmeans.labels_.reshape(num_rows, num_cols), interpolation='nearest', cmap='tab20')\n",
    "    return [im]\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=50, interval=20, blit=True) \n",
    "anim.save('kmeans_nnunet_calcium.mp4', writer='ffmpeg', fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radio Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_version=\"radio_v2.1\" # for RADIO\n",
    "#model_version=\"e-radio_v2\" # for E-RADIO\n",
    "model = torch.hub.load('NVlabs/RADIO', 'radio_model', version=model_version, progress=True, skip_validation=True)\n",
    "model.cuda().eval()\n",
    "# x = torch.rand(1, 3, 224, 224, device='cuda')\n",
    "\n",
    "if \"e-radio\" in model_version:\n",
    "    model.model.set_optimal_window_size(x.shape[2:]) #where it expects a tuple of (height, width) of the input image.\n",
    "\n",
    "preprocessor = model.make_preprocessor_external()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# RADIO expects the input to have values between [0, 1]. It will automatically normalize them to have mean 0 std 1.\n",
    "# image = cv2.imread('/home/naravich/projects/RADIO/assets/radio.png', cv2.IMREAD_COLOR)\n",
    "image = cv2.imread('/home/naravich/projects/RADIO/vis_denoise/02005Pre/02005Pre_001.png', cv2.IMREAD_COLOR)\n",
    "# image = image[80:420, 80:420, :]\n",
    "print(image.shape)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_size = (512, 512)\n",
    "image = cv2.resize(image, image_size)\n",
    "image = image / 255.0\n",
    "images = processor(image, return_tensors=\"pt\")\n",
    "# image = np.transpose(image, (2, 0, 1))\n",
    "# images = image[np.newaxis, ...]\n",
    "# images = torch.tensor(images, device='cpu', dtype=torch.float32)\n",
    "# plt.imshow(images[0].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "# x = images.to(device)\n",
    "# with torch.autocast(device.type, dtype=torch.bfloat16):\n",
    "#     summary, spatial_features = model(x)\n",
    "spatial_features = model(images['pixel_values']).last_hidden_state\n",
    "print(spatial_features.shape, len(spatial_features))\n",
    "# plt.imshow(spatial_features.cpu().squeeze(dim=0).detach().numpy(), cmap='hot', interpolation='nearest')\n",
    "# plt.imshow(spatial_features[0].transpose(1, 2, 0) * 255, cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['pixel_values'].min(), images['pixel_values'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = num_cols = 16\n",
    "feat = spatial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = spatial_features.cpu().detach()\n",
    "num_rows = image_size[-2] // 16\n",
    "num_cols = image_size[-1] // 16\n",
    "feat = rearrange(f, 'b (h w) c -> b h w c', h=num_rows, w=num_cols).float()\n",
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = spatial_features.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig = plt.figure()\n",
    "# labeling the x-axis and y-axis \n",
    "axis = plt.axes() \n",
    "\n",
    "def animate(frame_number):\n",
    "    # kmeans = KMeans(n_clusters=frame_number + 1, random_state=0).fit(rearrange(feat, 'b h w c -> (b h w) c', h=num_rows, w=num_cols).numpy())\n",
    "    kmeans = KMeans(n_clusters=frame_number + 1, random_state=0).fit(feat.reshape(num_cols * num_rows, -1).numpy())\n",
    "\n",
    "    axis.set_title(f'K: {frame_number}')\n",
    "    im = axis.imshow(kmeans.labels_.reshape(num_rows, num_cols), interpolation='nearest', cmap='tab20')\n",
    "    return [im]\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=50, interval=20, blit=True) \n",
    "anim.save('kmeans_segformer.mp4', writer='ffmpeg', fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feat = [feat]\n",
    "all_feat = list(zip(*all_feat))\n",
    "for i, feats in enumerate(all_feat):\n",
    "    colored = []\n",
    "    for features in feats:\n",
    "        # color = get_pca_map(features, images.shape[-2:])\n",
    "        color = get_pca_map(features, images.shape[-2:], interpolation='bilinear')\n",
    "        colored.append(color)\n",
    "\n",
    "    orig = cv2.cvtColor(images[i].permute(1, 2, 0).cpu().numpy(), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # cv2.imwrite(f'{dirs[\"orig\"]}/vis_{ctr}.jpg', orig * 255)\n",
    "    # cv2.imwrite(f'{dirs[\"viz\"]}/vis_{ctr}.jpg', colored[-1] * 255)\n",
    "\n",
    "    op = np.concatenate([orig] + colored, axis=1) * 255\n",
    "    orig = orig * 255\n",
    "    viz = colored[-1] * 255\n",
    "\n",
    "    # cv2.imwrite(f'{dirs[\"sbs\"]}/vis_{ctr}.jpg', op)\n",
    "    sbs = op\n",
    "    # ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sbs[:, :, ::-1] / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.min(), viz.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('sbs.jpg', sbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz[:, :, ::-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(viz[:, :, ::-1] / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
